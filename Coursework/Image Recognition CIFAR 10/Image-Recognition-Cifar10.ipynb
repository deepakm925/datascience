{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A link to the dataset can be found here:\n",
    "http://www.cs.utoronto.ca/%7Ekriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset contains pictures of objects belonging to 10 categories or class: \n",
    "\n",
    "labels: airplane, automobile, bird, cat, deer, dog, frog, horse, ship truck\n",
    "\n",
    "The dataset consists of 60,000 32 x 32 color images, with 6000 images per class.  There are a total of 3072 properties or features present for each image (32 * 32 * 3 = 3072) where each 32 integer represents the width and height of each image in pixels.  This figure is then multiplied by 3 because the images have three color channels, red, green and blue.  The training data features comes in the form of a multi-dimensional array, with 50,000 rows and 3072 columns, where each row is a uint8 representation an image.  The test set features is also a multi-dimensional array, consisting of 10,000 rows and 3072 columns.  Both training and testing datasets contains labels in a one dimensional format, where values for the ith label can range between 0-9.  Each image is stored in row-major order, where the first 1024 indices represent the values of the red channel, the next 1024 the values of the green channel, and the last 1024 the values of the blue channel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0\n",
      "0    airplane\n",
      "1  automobile\n",
      "2        bird\n",
      "3         cat\n",
      "4        deer\n",
      "5         dog\n",
      "6        frog\n",
      "7       horse\n",
      "8        ship\n",
      "9       truck\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "\n",
    "def unpickle(file):\n",
    "    \n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "meta_file = 'Datasets/Cifar/batches.meta'\n",
    "meta_data = unpickle(meta_file)\n",
    "categories = pd.DataFrame.from_dict(meta_data['label_names'])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of what the training and test sets looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  43  50 ..., 140  84  72]\n",
      " [154 126 105 ..., 139 142 144]\n",
      " [255 253 253 ...,  83  83  84]\n",
      " ..., \n",
      " [ 71  60  74 ...,  68  69  68]\n",
      " [250 254 211 ..., 215 255 254]\n",
      " [ 62  61  60 ..., 130 130 131]]\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "def load_file(batch_1 = None, batch_2 = None, batch_3 = None, batch_4 = None, batch_5 = None, test_batch = None):\n",
    "    \n",
    "    # Load training batch and test set   \n",
    "    file_1 = open(batch_1, 'rb')\n",
    "    training_batch = cPickle.load(file_1)\n",
    "    file_1.close()\n",
    "    \n",
    "    test = open(test_batch, 'rb')\n",
    "    test_set = cPickle.load(test)\n",
    "    test.close()\n",
    "    \n",
    "    # training data\n",
    "    training_features_sample = training_batch['data']\n",
    "    print(training_features_sample)\n",
    "    #print(training_features_sample.shape)\n",
    "    \n",
    "    # training data shape\n",
    "    #print(training_batch['data'].shape)\n",
    "    \n",
    "    # sample of 200 labels\n",
    "    #print(test_set['labels'][:200])\n",
    "    return training_features_sample\n",
    "\n",
    "training_batch_1_path = 'Datasets/Cifar/data_batch_1'\n",
    "test_set_path = 'Datasets/Cifar/test_batch'\n",
    "X_train = load_file(batch_1 = training_batch_1_path, test_batch = test_set_path)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and specify global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import h5py\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.advanced_activations\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "lambda_reg = 0.001\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# the CIFAR10 images are RGB\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  A function that returns training, validation and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was taken from keras' library of preprocessed datasets.  The dataset has already been reshaped and returns T4 tensors (4 dimensional arrays).  \n",
    "Eg. X_train = [number of rows, depth, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # Allocate last 5000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "    y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "    \n",
    "    # preprocess data\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_val = X_val.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    X_val /= 255\n",
    "    \n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "    print(y_train.shape[0], 'training labels')\n",
    "    print(y_test.shape[0], 'test labels')\n",
    "    print(X_val.shape[0], 'validation samples')\n",
    "    print(y_val.shape[0], 'validation labels')\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:  Includes metrics from each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class History(keras.callbacks.Callback):\n",
    "    \n",
    "    #  called at the beginning of each training epoch\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.training_loss = []\n",
    "        self.training_acc = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "    \n",
    "    #  called at the end of each epoch\n",
    "    def on_epoch_end(self, batch, logs = {}):\n",
    "        self.training_loss.append(logs.get('loss'))\n",
    "        self.training_acc.append(logs.get('acc'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        \n",
    "        # Uncomment to save results after each epoch\n",
    "        \n",
    "        #np.save(file = 'Model/2nd_run/performance/training_loss', arr = self.training_loss)\n",
    "        #np.save(file = 'Model/2nd_run/performance/training_acc', arr = self.training_acc)    \n",
    "        #np.save(file = 'Model/2nd_run/performance/val_loss', arr = self.val_loss)   \n",
    "        #np.save(file = 'Model/2nd_run/performance/val_acc', arr = self.val_acc) \n",
    "        \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to plot and save the loss data \n",
    "\n",
    "def plot_loss(history = None, filepath = None):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.losses, color = 'red')\n",
    "    plt.plot(history.val_loss, color = 'blue')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title('Graph Comparing Testing and Validation Loss')\n",
    "    red_patch = mpatches.Patch(color='red', label='Training Loss')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Validation Loss')\n",
    "    plt.legend(handles=[red_patch, blue_patch],loc = 1)\n",
    "    plt.show()\n",
    "    if filepath:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "# function to plot and save the accuracy data\n",
    "\n",
    "def plot_accuracy(history, filepath):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.accuracy, color = 'red')\n",
    "    plt.plot(history.val_acc, color = 'blue')\n",
    "    plt.tight_layout() # adjusts labels so they fit into the figure area\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title('Graph Comparing Testing and Validation Accuracy')\n",
    "    red_patch = mpatches.Patch(color='red', label='Training Accuracy')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Validation Accuracy')\n",
    "    plt.legend(handles=[red_patch, blue_patch],loc = 4)\n",
    "    plt.show()\n",
    "    if filepath:\n",
    "        plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ConvNet was trained using stochastic gradient descent (SGD) with Nesterov momentum.  The momentum paramter was set to .9. \n",
    "Early stopping was added with enough room to learn but stop the model from training if it has stopped learning.  The training set data was shuffled after each batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test = None, y_test = None, model = None):\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(loss, accuracy)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test and predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 3, 32, 32)\n",
      "40000 train samples\n",
      "10000 test samples\n",
      "40000 training labels\n",
      "10000 test labels\n",
      "10000 validation samples\n",
      "10000 validation labels\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_translation_opt = {}\n",
    "\n",
    "def train_predict(cnn = None, learning_rate = None, translation = None, \n",
    "                  zca_whitening = False, rotation = None, flipping = None, shear_range = None,\n",
    "                  training_size = None, validation_size = None, filepath = None, weights_path = None):\n",
    "\n",
    "    print('cnn is %', cnn)\n",
    "    print('learning rate is %', learning_rate)\n",
    "    print('translation is %', translation)\n",
    "    print('rotation range is 0 - %', rotation)\n",
    "    print('flipping is %', flipping)\n",
    "    print('training size is %', training_size)\n",
    "    print('validation size is %', validation_size)\n",
    "    print('filepath is %', filepath)\n",
    "    \n",
    "    # instance of Stochastic Gradient Descent optimizer\n",
    "    sgd = SGD(lr = learning_rate, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "    \n",
    "    # set optimizer and specify cost function\n",
    "    cnn.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = sgd,\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    # configure data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=zca_whitening,  # apply ZCA whitening\n",
    "            rotation_range = rotation,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            shear_range = shear_range,\n",
    "            width_shift_range = translation,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range = translation,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip = flipping,  # randomly flip images\n",
    "            vertical_flip = False)  # randomly flip images\n",
    "    \n",
    "    if weights_path == None:\n",
    "        fit(model = cnn, datagen = datagen, training_size = training_size, \n",
    "                      validation_size = validation_size, filepath=filepath)\n",
    "    loss, accuracy = predict(X_test[:validation_size], y_test[:validation_size], model = cnn)\n",
    "    \n",
    "    key = str(translation)\n",
    "    accuracy_translation_opt[key] = accuracy\n",
    "    plot_accuracy(history=history)\n",
    "    plot_loss(history=history)\n",
    "    print('Accuracy and loss with {} translation = {} and {} respectively'.format(translation, accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(model = None, datagen = None, training_size = None, \n",
    "                  validation_size = None, filepath = None):\n",
    "    \n",
    "    # CALLBACKS:\n",
    "    \n",
    "    # Early stopping callback setup \n",
    "    early_stopping_val_acc = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                                 patience= 10, \n",
    "                                                                 verbose = 1, \n",
    "                                                                 mode = 'auto')\n",
    "    \n",
    "    # Weights callback setup\n",
    "    checkpoint_weights_path = 'Model/2nd_run/checkpoint/weights.h5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath = checkpoint_weights_path, \n",
    "                                                 monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    \n",
    "    #  Check if data augmentation was applied\n",
    "    if datagen:\n",
    "        print('Data augmentation applied')\n",
    "        datagen.fit(X_train[:training_size])\n",
    "        model.fit_generator(datagen.flow(X_train[:training_size], y_train[:training_size], batch_size = batch_size),\n",
    "                            samples_per_epoch = training_size, \n",
    "                            nb_epoch = nb_epoch,\n",
    "                            callbacks=[history, early_stopping_val_acc, checkpoint],\n",
    "                            validation_data=(X_val[:validation_size], y_val[:validation_size]))\n",
    "    else:\n",
    "        print('No data augmentation applied')\n",
    "        model.fit(X_train[:training_size], y_train[:training_size], \n",
    "             batch_size = batch_size, \n",
    "             nb_epoch = nb_epoch, \n",
    "             verbose = 1, \n",
    "             callbacks = [history, early_stopping_val_acc, checkpoint], \n",
    "             validation_data = (X_val[:validation_size], y_val[:validation_size]), \n",
    "             shuffle = True)\n",
    "    \n",
    "    # if filepath is specified, save it\n",
    "    if filepath: \n",
    "        print('yes')\n",
    "        model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convnet(weights_path=None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,32,32)))\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    model.add(Convolution2D(64, 3, 3, init='he_normal', activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    # Convolutional layer 2\n",
    "    model.add(Convolution2D(64, 3, 3, init='he_normal', activation='relu'))\n",
    "    # MaxPooling Layer 1\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    # Convolutional layer 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, init='he_normal', activation='relu'))\n",
    "    # Convolutional layer 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, init='he_normal', activation='relu'))\n",
    "    # MaxPooling Layer 2\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    # Convolutional layer 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 6\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 7\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 8\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    # MaxPooling layer 3\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Convolutional layer 9\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 10\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 11\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 12\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # MaxPooling layer 4\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Convolutional layer 13\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 14\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 15\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Convolutional layer 16\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # MaxPooling 5\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # Fully Connected Layer 1\n",
    "    model.add(Dense(1024, init='he_normal', W_regularizer = l2(.004), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Fully Connected Layer 2\n",
    "    model.add(Dense(1024, init='he_normal', W_regularizer = l2(.004), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Fully Connected Layer 3\n",
    "    model.add(Dense(10, init='he_normal', activation='softmax'))\n",
    "    \n",
    "    # If saved weights exist, load it\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "        print('loaded pre-trained model')\n",
    "        \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_weights = <Specify a filepath to save the weights here>\n",
    "\n",
    "# instance of model\n",
    "convNet = cuda_conv()\n",
    "\n",
    "# fit and predic\n",
    "train_predict(cnn=convNet,\n",
    "              learning_rate= .01,\n",
    "              translation=.0625,\n",
    "              flipping=True,\n",
    "              rotation=0,\n",
    "              zca_whitening = False,\n",
    "              featurewise_center = False,\n",
    "              featurewise_std_normalization = False,\n",
    "              shear_range=None,\n",
    "              training_size=len(X_train),\n",
    "              validation_size=len(X_val),\n",
    "              filepath=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
